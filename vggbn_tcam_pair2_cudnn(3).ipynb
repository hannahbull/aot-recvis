{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwjwfQlwAnWk"
   },
   "source": [
    "install pytorch on GCP: https://discuss.pytorch.org/t/deploy-pretrained-pytorch-model-on-google-cloud/14664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "qjehU2vA8bK6",
    "outputId": "da8edb80-620c-4e1b-e9fe-9f25caad27bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6STyjjmYStQR",
    "outputId": "d4f95846-4448-48e2-f24a-d6e70686882f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7401"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "us3qa_GyAnXV",
    "outputId": "e3c79874-2e28-437e-809b-98c10395ba94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blabla_a'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'blabla'+'_a'*True +'_b'*False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7xlRrdXdxX02"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nWcnVUpx5j-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import argparse\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import glob\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import numpy as np\n",
    "#import torchvision\n",
    "#import dataloader # LISA: dataloader is a folder of the github repo of two-stream conv fusion. I have copied all the necessary function above.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import skimage\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_RRZXJcsAnXw"
   },
   "outputs": [],
   "source": [
    "# parameters for temporal chuking:\n",
    "temporal_chunking = True \n",
    "if temporal_chunking:\n",
    "    L = 6 \n",
    "else:\n",
    "    L = 40 #this can't be modified since there is only 40 frames per video !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "It3hg3-OzIj9"
   },
   "source": [
    "####  two-stream-fusion-for-action-recognition-in-videos/dataloader/split_train_test_video.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q3_EoamDAnX9",
    "outputId": "718caf66-861d-4ecd-a55c-4020861cab32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = '/data/vision/billf/donglai-lib/Data/High/Classify/UCF101/bflowb_orig/BoxingPunchingBag/v_BoxingPunchingBag_g02_c02 280 1'\n",
    "a.split('/',11)[-1].split(' ')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9ruqTlPyd02"
   },
   "outputs": [],
   "source": [
    "class UCF101_splitter():\n",
    "    def __init__(self, path, split):\n",
    "        self.path = path # /train_val_split ?\n",
    "        self.split = split\n",
    "\n",
    "    \n",
    "    \n",
    "    def split_video(self):\n",
    "        train_video = self.file2_dic(self.path+'train01_cnn_ta_flow_orig_fb.txt')\n",
    "        test_video = self.file2_dic(self.path+'test01_cnn_ta_flow_orig_fb.txt')\n",
    "        print('==> (Training video, Validation video):(', len(train_video),len(test_video),')')\n",
    "        self.train_video = self.make_dict(train_video)\n",
    "        self.test_video = self.make_dict(test_video)\n",
    "\n",
    "        return self.train_video, self.test_video\n",
    "\n",
    "    def file2_dic(self, fname):\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "            content = [x.strip('\\r\\n') for x in content]\n",
    "        f.close()\n",
    "        dic = {}\n",
    "        with open('video_labels.pickle', 'rb') as handle:\n",
    "            video_labels = pickle.load(handle)\n",
    "        handle.close()\n",
    "        count = 0\n",
    "        for line in content:\n",
    "            video_name = line.split('/',6)[-2]\n",
    "            video_class = line.split('/',6)[-1].split(' ')[0]\n",
    "\n",
    "            #print('video_name',video_name)\n",
    "            #print('video_class: ', video_class)\n",
    "            \n",
    "            video_name_class = video_name+'_' +video_class #b'*(video_class == 0)+'_f'*(video_class == 1)\n",
    "            #print('video_name_class: ',video_name_class)\n",
    "\n",
    "            #print('video_labels[video_name_class]: ', video_labels[video_name_class])\n",
    "            try:\n",
    "                #print('video_name_class[2:]: ', video_name_class[:2])\n",
    "                #print('video_name_class: ',video_name_class)\n",
    "                dic[video_name_class[2:]] = video_labels[video_name_class]\n",
    "                \n",
    "            except: \n",
    "                count+=1\n",
    "        print('they are '+str(count)+' missing videos')\n",
    "                \n",
    "        return(dic)\n",
    "    \n",
    "    def file2_dic_(self,fname): \n",
    "        # here we keep the same split test train as in th text file for action recognition...\n",
    "        # do we need to keep both backward and forward for the training ?\n",
    "        with open(fname) as f:\n",
    "            content = f.readlines()\n",
    "            content = [x.strip('\\r\\n') for x in content]\n",
    "#            print('file2_dic / content[:5]', content[:5])\n",
    "        f.close()\n",
    "        with open('video_labels.pickle', 'rb') as handle:\n",
    "            video_labels = pickle.load(handle)\n",
    "        handle.close()\n",
    "        dic={}\n",
    "        count = 0\n",
    "        \n",
    "        for line in content:\n",
    "            #print line\n",
    "            video = line.split('/',1)[1].split(' ',1)[0]\n",
    "            key = video.split('_',1)[1].split('.',1)[0]\n",
    "            #print('key+\\'_b\\'',key+'_b')\n",
    "            #label = self.action_label[line.split('/')[0]]   \n",
    "            #print('v'+key+'_b')\n",
    "            arrow = np.random.randint(2)\n",
    "            if arrow == 0:\n",
    "                try:\n",
    "                    dic[key+'_b'] = video_labels['v_'+key+'_b']\n",
    "                #print('dic[key+\\'_b\\'],dic[key+\\'_f\\']',dic[key+'_b'],dic[key+'_f'])\n",
    "                except:\n",
    "                    count+=1\n",
    "            else:\n",
    "                try:\n",
    "                    dic[key+'_f'] = video_labels['v_'+key+'_f']\n",
    "                except:\n",
    "                    count+=1\n",
    "        print('they are '+str(count)+' missing videos')\n",
    "            \n",
    "            #dic[key] = int(label)\n",
    "            #print key,label\n",
    "        return dic\n",
    "\n",
    "    def make_dict(self,dic):\n",
    "        dic2 = {}\n",
    "        count = 0\n",
    "        for video in dic:\n",
    "            #if video.split('_')[0]!='HandStandPushups': \n",
    "                # LISA: [correction] because in the frame_count.pickle file, there is no any \"HandStandPushups\" video\n",
    "            dic2[video] = float(dic[video]) # LISA: [correction] it was written dic2[videoname] = dic[video] but videoname is not assigned...\n",
    "#                if count <5:\n",
    "#                    print(video, dic[video])\n",
    "#                count +=1\n",
    "        return dic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Udj1_VljAnYi",
    "outputId": "1368c172-4797-4b34-fa3f-e60f95d6bdb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_GolfSwing_g21_c02_b 0\n",
      "v_PlayingDaf_g13_c02_b 0\n",
      "v_TrampolineJumping_g23_c04_b 0\n",
      "v_Nunchucks_g22_c02_b 0\n",
      "v_Fencing_g02_c04_f 1\n"
     ]
    }
   ],
   "source": [
    "with open('video_labels.pickle', 'rb') as handle:\n",
    "            video_labels = pickle.load(handle)\n",
    "handle.close()\n",
    "\n",
    "for k in list(video_labels.keys())[:5]:\n",
    "    print(k,video_labels[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lg8uanXUy1qr"
   },
   "source": [
    "####  two-stream-fusion-for-action-recognition-in-videos/dataloader/temp_loader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbMzjHdoyUY5"
   },
   "outputs": [],
   "source": [
    "class motion_dataset(Dataset):  \n",
    "    def __init__(self, dic, in_channel, root_dir, mode, transform=None):\n",
    "        self.keys=dic.keys()\n",
    "        self.values=dic.values()\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.mode=mode\n",
    "        self.in_channel = in_channel # be careful we only have 40 frames...\n",
    "        self.img_rows=224\n",
    "        self.img_cols=224\n",
    "\n",
    "    def stackopf(self):\n",
    "        name = 'v_'+self.video\n",
    "        u = self.root_dir+ name + '/x'\n",
    "        v = self.root_dir+ name+ '/y'\n",
    "        #print(u,v)\n",
    "        \n",
    "        flow = torch.FloatTensor(2*self.in_channel,self.img_rows,self.img_cols)\n",
    "        i = int(self.clips_idx)\n",
    "#        print(i)\n",
    "\n",
    "\n",
    "        for j in range(self.in_channel):        \n",
    "            idx = i + j #LISA: i + j since in_channel = 50 and num_frames = O(50)\n",
    "            idx = str(idx)\n",
    "            #frame_idx = 'frame'+ idx.zfill(6)\n",
    "            frame_idx = idx.zfill(4)\n",
    "            h_image = u +'/' + frame_idx +'.jpg'\n",
    "            v_image = v +'/' + frame_idx +'.jpg'\n",
    "            \n",
    "            imgH=(Image.open(h_image))\n",
    "            imgV=(Image.open(v_image))\n",
    "\n",
    "            H = self.transform(imgH)\n",
    "            V = self.transform(imgV)\n",
    "\n",
    "            \n",
    "            flow[2*(j-1),:,:] = H\n",
    "            flow[2*(j-1)+1,:,:] = V      \n",
    "            imgH.close()\n",
    "            imgV.close()  \n",
    "        return flow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):  ### LISA: what is idx ?? is it a list ?\n",
    "        #print ('mode:',self.mode,'calling Dataset:__getitem__ @ idx=%d'%idx)\n",
    "        \n",
    "        if self.mode == 'train' or 'val':\n",
    "            ########\n",
    "            # LISA #\n",
    "            ########\n",
    "            # here is the call to the videos\n",
    "            # question, when do we pass the forward and backward videos ? can we pass it throw the same batch?\n",
    "            \n",
    "            #self.video, nb_clips = list(self.keys)[idx].split('-') #LISA: [correction] self.keys[idx] doesn't work\n",
    "            self.video = list(self.keys)[idx]\n",
    "            self.clips_idx = 1\n",
    "        else:\n",
    "            raise ValueError('There are only train and val mode')\n",
    "\n",
    "        #print('idx: ', idx, type(idx))\n",
    "        \n",
    "        label = float(list(self.values)[idx]) #LISA: [correction] self.values[idx] doesn't work\n",
    "        #print('label: ',label)\n",
    "        #print('int(label): ',int(label))\n",
    "        #label = float(label) \n",
    "        #print('label: ',label, type(label))\n",
    "        data = self.stackopf()\n",
    "\n",
    "        if self.mode == 'train' or 'val':\n",
    "            sample = (data,label)\n",
    "        else:\n",
    "            raise ValueError('There are only train and val mode')\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIGRB2wdxVYR"
   },
   "outputs": [],
   "source": [
    "class Motion_DataLoader():\n",
    "    def __init__(self, BATCH_SIZE, num_workers, in_channel,  path, ucf_list, ucf_split,train_transform,val_transform):\n",
    "\n",
    "        self.BATCH_SIZE=BATCH_SIZE\n",
    "        self.num_workers = num_workers\n",
    "        self.frame_count={}\n",
    "        self.in_channel = in_channel\n",
    "        self.data_path=path\n",
    "        self.path_ucf = ucf_list\n",
    "        self.train_transform = train_transform\n",
    "        self.val_transform = val_transform\n",
    "        # split the training and testing videos\n",
    "        splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n",
    "        self.train_video, self.test_video = splitter.split_video()\n",
    "        \n",
    "    def load_frame_count(self):\n",
    "        #loading the frame count for videos\n",
    "        with open(self.path_ucf+'/frame_count.pickle','rb') as file:\n",
    "            dic_frame = pickle.load(file)\n",
    "        file.close()\n",
    "#         print(\"Now in loadframe_count \")\n",
    "\n",
    "        for line in dic_frame : # LISA: removing the v_ and the .avi\n",
    "            videoname = line.split('_',1)[1].split('.',1)[0]\n",
    "            n,g = videoname.split('_',1)\n",
    "            self.frame_count[videoname]=dic_frame[line]\n",
    "\n",
    "    def run(self):\n",
    "        #self.load_frame_count()\n",
    "        #self.get_training_dic()\n",
    "        #self.val_sample()\n",
    "        train_loader = self.train()\n",
    "        val_loader = self.val()\n",
    "\n",
    "        return train_loader, val_loader, self.test_video\n",
    "            \n",
    "    def train(self):\n",
    "        training_set = motion_dataset(dic=self.train_video, in_channel=self.in_channel, root_dir=self.data_path,\n",
    "        #training_set = motion_dataset(dic=self.dic_video_train, in_channel=self.in_channel, root_dir=self.data_path,\n",
    "            mode='train',\n",
    "            transform = self.train_transform)\n",
    "        #transforms.Compose([\n",
    "        #    transforms.Resize([224,224]),\n",
    "        #    transforms.ToTensor(),\n",
    "        #    ])\n",
    "        print('==> Training data :',len(training_set),' videos',training_set[1][0].size())\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_set, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "            )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val(self):\n",
    "        validation_set = motion_dataset(dic= self.test_video, in_channel=self.in_channel, root_dir=self.data_path ,\n",
    "        #validation_set = motion_dataset(dic= self.dic_test_idx, in_channel=self.in_channel, root_dir=self.data_path ,\n",
    "            mode ='val',\n",
    "            transform =self.val_transform)\n",
    "         #transforms.Compose([\n",
    "            #transforms.Resize([224,224]),\n",
    "            #transforms.ToTensor(),\n",
    "            #])\n",
    "        print('==> Validation data :',len(validation_set),' frames',validation_set[1][0].size())\n",
    "        #print validation_set[1]\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            dataset=validation_set, \n",
    "            batch_size=self.BATCH_SIZE, \n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers)\n",
    "\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyytqopqzvPH"
   },
   "source": [
    "# Two-Stream convolutional fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUoboHU66cFN"
   },
   "source": [
    "## plots: \n",
    "> ### confusion matrix, images.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dLM-WUn5sO-"
   },
   "outputs": [],
   "source": [
    "# for computing confusion matrix\n",
    "def compute_confusion_matrix(y_actu, y_pred, class_names):\n",
    "    # Compute confusion matrix\n",
    "    cnf_matrix = confusion_matrix(y_actu, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "    # Plot non-normalized confusion matrix\n",
    "    #uncomment below lines if you want to plot non-normalized matrix\n",
    "    #plt.figure()\n",
    "    #plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#for plotting confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    \n",
    "#function for showing the inputs at the time of loading of data\n",
    "def show_images(inputs,label):\n",
    "    for i in range(inputs.shape[0]):\n",
    "        a = inputs[i:i+1,:]\n",
    "        a = a.view([int(inputs.size(1)/3),3,224,224])\n",
    "\n",
    "        out = torchvision.utils.make_grid(a)\n",
    "        out = out.numpy().transpose((1, 2, 0))\n",
    "        out = [0.229, 0.224, 0.225] * out + [0.485, 0.456, 0.406]\n",
    "        out = np.clip(out, 0, 1)\n",
    "        plt.imshow(out)\n",
    "        plt.pause(0.001) \n",
    "        print(\"Patches of RGB frames for batch %d for class %d\" % (i,label[i]+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3SqWR6O5t9O"
   },
   "source": [
    "## functions to adapt the backbone model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fv1F-AHexVSi"
   },
   "outputs": [],
   "source": [
    "#function for getting weights of first layer of pre-trained model and first layer of model defined on scratch\n",
    "def get_weights(trained_model, untrained_model):\n",
    "    i = 0\n",
    "    for param in untrained_model.parameters():\n",
    "        if(i == 0):\n",
    "            untrained_weights = param.data\n",
    "        i += 1\n",
    "        \n",
    "    i = 0\n",
    "    for param in trained_model.parameters():\n",
    "        if(i == 0):\n",
    "            trained_weights = param.data\n",
    "        i += 1 \n",
    "        \n",
    "    return [trained_weights, untrained_weights]\n",
    "    \n",
    "\n",
    "#function for appending the trained model weights to the first layer of model defined on scratch\n",
    "def concat_trained_weights(trained_weights, untrained_weights):\n",
    "    #averaging of trained filter weights\n",
    "    temp           = torch.sum(trained_weights[:,:], dim= 1)[0] / 3\n",
    "\n",
    "    filter_dim     = untrained_weights.size(1)\n",
    "    filter_weights = temp.view(1,3,3)\n",
    "    for i in range(filter_dim-1):\n",
    "        filter_weights = torch.cat((filter_weights, temp.view(1,3,3)), dim=0)\n",
    "\n",
    "    for i in range(untrained_weights.size(0)):\n",
    "        untrained_weights[i,:,:,:] = filter_weights\n",
    "    \n",
    "    return untrained_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUm4zcqd5lCY"
   },
   "source": [
    "## training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LC1U3NFdxVL_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Function for training of model\n",
    "This function can be modified accordingly to include validation set\n",
    "'''\n",
    "    \n",
    "def train_model(model, criterion, optimizer,scheduler, num_epochs, phase, temporal_chunking = False, L = 6): \n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        #for phase in ['train']:\n",
    "        j = 0\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        correct_samples = 0\n",
    "        batch_num = 0\n",
    "        # Iterate over data.\n",
    "        for i_data,data in enumerate(fullloader[phase]):  ## LISA: fullloader is a global variable created when \"loading the train and test data\"\n",
    "            #print('loading the data for batch: ', batch_num)\n",
    "            #print('i_data,data: ',i_data,data)\n",
    "            batch_num+=1\n",
    "            temp_data, labels = data \n",
    "\n",
    "            if use_gpu:\n",
    "                temp_data = Variable(temp_data.cuda())\n",
    "                labels    = Variable(labels.cuda())\n",
    "            else:\n",
    "                temp_data = Variable(temp_data)\n",
    "                labels    = Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            #temporal_chunking to fill before !\n",
    "            #outputs = model(temp_data, temporal_chunking, tau, )\n",
    "            outputs = model(temp_data, temporal_chunking = temporal_chunking, L = L)\n",
    "            #print('outputs: ', outputs.size())\n",
    "            #print('th: ', torch.Tensor([0.5]*fullloader[phase].batch_size))\n",
    "            #print('(Variable(torch.Tensor([0.5]*fullloader[phase].batch_size)) > outputs.cpu()): ', (Variable(torch.Tensor([0.5]*fullloader[phase].batch_size)) > outputs.cpu()))\n",
    "                \n",
    "            th = Variable(torch.Tensor([0.5]*outputs.size(0)))\n",
    "            outputs_variable = Variable(outputs)\n",
    "                #print('outputs_variable: ', outputs_variable)\n",
    "                #print('th: ', th)\n",
    "            preds = (th < outputs_variable.cpu()).float() * 1\n",
    "            #print('preds: ',preds)\n",
    "                #_,preds = torch.max(outputs, 1)  \n",
    "                #print('preds: ',preds)\n",
    "                #pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            del temp_data\n",
    "                \n",
    "            # backward and optimize for training mode\n",
    "            loss = criterion(outputs.float(), labels.float())\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('loss for batch {0} in epoch {1}: {2}'.format(batch_num, epoch,loss.item()))\n",
    "            # finding epoch losses and accuracies for fusion\n",
    "            epoch_loss += loss.item() #* spat_data.size(0)\n",
    "\n",
    "            #correct_samples += torch.sum(preds == labels.data)\n",
    "                \n",
    "            correct_samples += preds.eq(labels.float().cpu().data.view_as(preds)).cpu().sum()\n",
    "            del labels\n",
    "            #print(\"Correct Samples = \", correct_samples)\n",
    "                                         \n",
    "        phase_size = len(fullloader[phase])*fullloader[phase].batch_size\n",
    "#             print(\"phase size = \", phase_size)\n",
    "        epoch_acc = correct_samples / phase_size \n",
    "        epoch_loss = epoch_loss / len(fullloader[phase])\n",
    "        \n",
    "        if phase == 'train':\n",
    "            if epoch % 10 == 0: # \n",
    "                torch.save(model.state_dict(), 'models_pth/training_epoch_{0}.pth'.format(epoch))\n",
    "\n",
    "            #print(\"=================== Training mode ====================\")\n",
    "        #print('Loss: {:.4f} '.format(\n",
    "                #epoch_loss))\n",
    "        \n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        epoch_loss, correct_samples, phase_size,\n",
    "        100. * correct_samples / phase_size))\n",
    "        #batches_done = fullloader[phase].batch_size*(epoch - 1) + outputs.size(0)\n",
    "        #batches_left = fullloader[phase]*(n_epochs - epoch) + num_epochs - self.batch \n",
    "        #sys.stdout.write('ETA: %s' % (datetime.timedelta(seconds=batches_left*self.mean_period/batches_done)))\n",
    "            \n",
    "        acc = 100 * correct_samples /phase_size\n",
    "        \n",
    "    \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_epoch = epoch \n",
    "            print('*', end='')\n",
    "            # Save model onto disk.\n",
    "                \n",
    "            if phase == 'train':\n",
    "  \n",
    "                torch.save(model.state_dict(), 'models_pth/best_epoch.pth')\n",
    "    \n",
    "            print('Best at epoch %d, test accuaray %f' % (best_epoch, best_acc))\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best at epoch %d, test accuaray %f' % (best_epoch, best_acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qn_4TvLA5R95"
   },
   "source": [
    "## loading the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOk66dQq5RYR",
    "outputId": "642682b0-c5af-4814-bf4c-ff0a2874e3b8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they are 0 missing videos\n",
      "they are 0 missing videos\n",
      "==> (Training video, Validation video):( 14848 6364 )\n",
      "==> Training data : 14848  videos torch.Size([80, 224, 224])\n",
      "==> Validation data : 6364  frames torch.Size([80, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "#loading the train and test data\n",
    "'''\n",
    "change here for loading data for spatial loader, temporal loader\n",
    "\n",
    "'''\n",
    "#############################\n",
    "# ADAPTATION FOR TCAM MODEL #\n",
    "#############################\n",
    "\n",
    "# LISA: perform only the temporal loading\n",
    "# LISA: be careful with the data transformation: data augmentation but also artificial cues removing\n",
    "data_loader = Motion_DataLoader(BATCH_SIZE=128, num_workers=1, in_channel = 40, #LISA: why ??\n",
    "    #def __init__(self, BATCH_SIZE, num_workers, in_channel,  path, ucf_list, ucf_split)\n",
    "#data_loader = spatio_temporal_dataloader(BATCH_SIZE=16, num_workers=1, in_channel = 50, #LISA: why ??\n",
    "                                #spatial_path='/home/lisabedin/test/jpegs_256/',    \n",
    "                                path='/home/lisabedin/dataset_AoT/',\n",
    "                                ucf_list='/home/lisabedin/',# train_val_split #ucf_list\n",
    "                                ucf_split='02',\n",
    "                                train_transform = transforms.Compose([\n",
    "                                                   transforms.RandomCrop(224),\n",
    "                                                   transforms.RandomHorizontalFlip(),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                               ]),\n",
    "                                val_transform = transforms.Compose([\n",
    "                                                     transforms.Resize([224,224]),\n",
    "                                                     transforms.ToTensor(),\n",
    "                                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225])\n",
    "                                                 ]))\n",
    "train_loader, test_loader, test_video = data_loader.run()\n",
    "'''\n",
    "appending train-loader and test loader for training the model\n",
    "'''\n",
    "fullloader = {}\n",
    "fullloader['train'] = train_loader\n",
    "fullloader['val'] = test_loader\n",
    "del train_loader\n",
    "del test_loader\n",
    "\n",
    "'''\n",
    "loading classes name for computing confusion matrix\n",
    "'''\n",
    "temp = []\n",
    "with open(\"/home/lisabedin/UCF_list/classInd.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        abc = line.split(\" \")[1]\n",
    "        #print(abc)\n",
    "        temp.append(abc.rstrip(\"\\n\"))\n",
    "f.close()\n",
    "#print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJmDB2Gw6tZd"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BM3UQYIAnaH"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Defining a model model which will do convolution fusion of both stream and 3D pooling \n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ## LISA: spat_model and temp_model are global variable defined in the cell below \n",
    "        \n",
    "        self.temp_feature = temp_model.features #feature_size = Nx512x7x7\n",
    "        \n",
    "        self.pooling_temporal_chunks = nn.MaxPool3d(5,5)\n",
    "        \n",
    "        self.conv2d1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024,kernel_size=3, padding=1), #853\n",
    "            nn.BatchNorm2d(1024,1e-3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5))\n",
    "        \n",
    "        self.conv2d2 = nn.Sequential(nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(1024,1e-3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5))\n",
    "        \n",
    "        self.CAM_avg_pool = nn.AvgPool2d(14,14)#.cuda()\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(1024,1), # LISA: batch_size be careful !!\n",
    "            nn.Sigmoid())#.cuda()\n",
    "        \n",
    "    def forward(self, x, temporal_chunking = False, L = 6, T = 5):\n",
    "        #print('x input: ', x.size())\n",
    "        \n",
    "        # temporal chunking:\n",
    "        if temporal_chunking:\n",
    "            stride = int(np.random.randint(L))\n",
    "            #print('stride',stride)\n",
    "            starting_frame = int(np.random.randint(int(x.size(1)/2))) #x.size(1) = 80 since there is the flow according x and the flow acording y\n",
    "        \n",
    "            if starting_frame < L/2:\n",
    "                starting_frame = L/2 # this is the starting frame according one direction !!\n",
    "                \n",
    "            elif starting_frame + (T-1) * stride +2 >= x.size(1)/2 - L/2: ### T sampling optical flow stacks!!\n",
    "                starting_frame = x.size(1)/2 - L/2 - (T-1) * stride -2\n",
    "            \n",
    "            current_frame = int(2*starting_frame)\n",
    "            #print('starting_frame, current_frame: ', starting_frame,current_frame)\n",
    "            \n",
    "            #print('current frame 0: ',current_frame)\n",
    "            current_tensor = x[:,current_frame-L:current_frame+L+2,:,:]\n",
    "            #print('current tensor 0: ', current_tensor.size())\n",
    "            \n",
    "            y = self.temp_feature(current_tensor)\n",
    "            #print('temp stride 0: ', y.size())\n",
    "            for k in range(T-1):\n",
    "                current_frame = int(current_frame + 2*stride)\n",
    "                #print('current frame {0}: '.format(k+1),current_frame)\n",
    "                current_tensor = x[:,current_frame-L:current_frame+L+2,:,:]\n",
    "                #print('current tensor {0}: '.format(k+1), current_tensor.size())\n",
    "                #print('current_frame-L:current_frame+L+2: ',current_frame-L,current_frame+L+2)\n",
    "                \n",
    "                y = torch.cat((y,self.temp_feature(current_tensor)), dim= 1)\n",
    "                #print('temp cat '+str(k+1)+': ', y.size())\n",
    "                \n",
    "            y = self.pooling_temporal_chunks(y)\n",
    "            #print('pooling_temporal_chunks: ', y.size())\n",
    "        else: \n",
    "        \n",
    "            y = self.temp_feature(x)\n",
    "            #print('self.temp_feature(x): ',x.size())\n",
    "        \n",
    "        # CAM model for classification\n",
    "        y = self.conv2d1(y)\n",
    "        #print('self.conv2d1(x): ', x.size())\n",
    "        \n",
    "        y = self.conv2d2(y)\n",
    "        #print('self.conv2d2(x): ', x.size())\n",
    "\n",
    "        y = self.CAM_avg_pool(y)\n",
    "        #print('avg: ', x.size())\n",
    "          \n",
    "        y = y.view(-1,1024)\n",
    "        #print('x.view(-1,1024): ',x.size())\n",
    "        \n",
    "        y = self.classifier(y)\n",
    "        #print('self.classifier(x): ', x.size())\n",
    "        y = y.view(y.size(0))\n",
    "        #print('view: ', x.size())\n",
    "        #print('x: ',x)\n",
    "        #print('type(x): ', type(x))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFc1DkR_7doL"
   },
   "source": [
    "## Initialization of the model: \n",
    "\n",
    "> ### pretraining and adapatation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_fF0SZJAnaZ",
    "outputId": "a08b438a-96d0-4b6f-9ae7-62983475e243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16 loaded\n",
      "Training completed in 0m 3s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "using pre-trained weights on ImageNet for both streams\n",
    "'''\n",
    "#LISA: be careful, run this cell only one time otherwise you'll get GPU memory errors\n",
    "since = time.time()\n",
    "#spat_model = models.vgg16(pretrained=False) #LISA: change with pretrained=True (but longer)\n",
    "temp_model = models.vgg16(pretrained=False)\n",
    "print('vgg16 loaded')\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjkGxYsb5RAR",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "first average the weight value across the RGB channels \n",
    "and replicate this average by the channel number of model\n",
    "'''\n",
    "\n",
    "#changing input filter dimension of temporal model \n",
    "feat_     = list(temp_model.features.children())[:-1]# LISA: as in the T-CAM model removing all the layers after \n",
    "class_  = list(temp_model.classifier.children())\n",
    "if temporal_chunking:\n",
    "    feat_[0]  = nn.Conv2d(2*L+2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) \n",
    "else:\n",
    "    feat_[0]  = nn.Conv2d(2*L, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # 80= 2*40 channels\n",
    "class_[6] = nn.Linear(in_features=4096, out_features=101, bias=True)\n",
    "\n",
    "#temporary model \n",
    "abc = nn.Sequential(*feat_) \n",
    "\n",
    "[trained_weights, untrained_weights] = get_weights(temp_model, abc)\n",
    "trained_weights                      = concat_trained_weights(trained_weights, untrained_weights)\n",
    "\n",
    "i = 0\n",
    "for param in abc.parameters():\n",
    "    if(i == 0):\n",
    "        param.data = trained_weights\n",
    "    i += 1 \n",
    "    \n",
    "temp_model.features = abc\n",
    "temp_model.classifier = nn.Sequential(*class_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T6hDR5dC7tLT"
   },
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRF2rCdM7sTs",
    "outputId": "638e455e-4ffa-4cc5-8715-774ec0a500e8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 0/429\n",
      "----------\n",
      "outputs:  torch.Size([128])\n",
      "loss for batch 1 in epoch 0: 0.6973584294319153\n",
      "outputs:  torch.Size([128])\n",
      "loss for batch 2 in epoch 0: 0.6953518986701965\n",
      "outputs:  torch.Size([128])\n",
      "loss for batch 3 in epoch 0: 0.6941211223602295\n",
      "outputs:  torch.Size([128])\n",
      "loss for batch 4 in epoch 0: 0.6949287056922913\n",
      "outputs:  torch.Size([128])\n",
      "loss for batch 5 in epoch 0: 0.6942180395126343\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "if system has cuda \n",
    "'''\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(torch.cuda.memory_allocated())#device=None\n",
    "\n",
    "model   = Net()\n",
    "\n",
    "if use_gpu:\n",
    "    #model = model.cuda()\n",
    "    model = torch.nn.DataParallel(model,device_ids=[0,1,2,3]).cuda()\n",
    "    cudnn.benchmark=True\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss() #CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer,scheduler, num_epochs=430, phase='train', temporal_chunking = temporal_chunking,L=L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cWEVAjGCStXU"
   },
   "outputs": [],
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4NX9LxeGAnbX"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('models_pth/best_epoch.pth')\n",
    "model = Net()\n",
    "model.load_state_dict(state_dict)\n",
    "#model.eval()\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "model = train_model(model, criterion, optimizer,scheduler, num_epochs=1, phase='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "faQyz_T1Anbn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qM78J_6iAnbz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CVHiD0LDAnb8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "vggbn_tcam_pair2_cudnn(8).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
